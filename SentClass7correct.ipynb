{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of SentClass3 with 1lac training data- using Unidirectional LSTM . Gave 76% test accuracy\n",
    "\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/twitter_train_data_correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 is negative and 1 is positive\n",
    "data.loc[data['target'] == 4, 'target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['p_text'] = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    text = re.sub(re.compile(r'http\\S+'), \"\",text)\n",
    "    return text\n",
    "\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(re.compile(r'@\\S+'), \"\",text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "data['p_text'] = data['p_text'].apply(lambda x: remove_url(x))\n",
    "data['p_text'] = data['p_text'].apply(lambda x: remove_mentions(x))\n",
    "data['p_text'] = data['p_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#Now 'p_text' has processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run only once to find maximum length of whole set\n",
    "all_texts = np.array(data['p_text'])\n",
    "maxLen = 0\n",
    "for i in all_texts:\n",
    "    Len = len(i.split())  #problem1\n",
    "    if(Len > maxLen):\n",
    "        maxLen = Len\n",
    "maxLen += 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data.sample(n = 100000)\n",
    "val_set = data.sample(n = 20000)\n",
    "test_set = data.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(train_set[\"p_text\"])\n",
    "train_Y = np.array(train_set[\"target\"])\n",
    "val_X = np.array(val_set[\"p_text\"])\n",
    "val_Y = np.array(val_set[\"target\"])\n",
    "test_X = np.array(test_set[\"p_text\"])\n",
    "test_Y = np.array(test_set[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    all_keys = word_to_index.keys()\n",
    "    for i in range(m):                               \n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = maxLen-len(X[i].lower().split())\n",
    "        for w in sentence_words:\n",
    "            if w in all_keys:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            j = j+1\n",
    "  \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_indices = sentences_to_indices(train_X, word_to_index, maxLen)\n",
    "val_X_indices = sentences_to_indices(val_X, word_to_index, maxLen)\n",
    "test_X_indices = sentences_to_indices(test_X, word_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement) [unk token]\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len,emb_dim, trainable=False)\n",
    "    \n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "    embedding_layer.build((None,)) # Do not modify the \"None\".  This line of code is complete as-is.\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti(input_shape, word_to_vec_map, word_to_index):\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph.\n",
    "    # It should be of shape input_shape and dtype 'int32' (as it contains indices, which are integers).\n",
    "    sentence_indices = Input(shape=input_shape,dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer\n",
    "    # (See additional hints in the instructions).\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # The returned output should be a batch of sequences.\n",
    "    X = LSTM(128,return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # The returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128,return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    \n",
    "    # Propagate X through a Dense layer with 1 units\n",
    "    X = Dense(1)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('sigmoid')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 45, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 45, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 45, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,411\n",
      "Trainable params: 223,361\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = senti((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 20000 samples\n",
      "Epoch 1/50\n",
      "100000/100000 [==============================] - 387s 4ms/step - loss: 0.5701 - accuracy: 0.6999 - val_loss: 0.5259 - val_accuracy: 0.7319\n",
      "Epoch 2/50\n",
      "100000/100000 [==============================] - 376s 4ms/step - loss: 0.5108 - accuracy: 0.7480 - val_loss: 0.4840 - val_accuracy: 0.7631\n",
      "Epoch 3/50\n",
      "100000/100000 [==============================] - 374s 4ms/step - loss: 0.4826 - accuracy: 0.7682 - val_loss: 0.4734 - val_accuracy: 0.7710\n",
      "Epoch 4/50\n",
      "100000/100000 [==============================] - 371s 4ms/step - loss: 0.4621 - accuracy: 0.7807 - val_loss: 0.4967 - val_accuracy: 0.7605\n",
      "Epoch 5/50\n",
      "100000/100000 [==============================] - 362s 4ms/step - loss: 0.4444 - accuracy: 0.7908 - val_loss: 0.4599 - val_accuracy: 0.7848\n",
      "Epoch 6/50\n",
      "100000/100000 [==============================] - 362s 4ms/step - loss: 0.4273 - accuracy: 0.8022 - val_loss: 0.4676 - val_accuracy: 0.7745\n",
      "Epoch 7/50\n",
      "100000/100000 [==============================] - 363s 4ms/step - loss: 0.4113 - accuracy: 0.8117 - val_loss: 0.4530 - val_accuracy: 0.7885\n",
      "Epoch 8/50\n",
      "100000/100000 [==============================] - 363s 4ms/step - loss: 0.3947 - accuracy: 0.8209 - val_loss: 0.4681 - val_accuracy: 0.7849\n",
      "Epoch 9/50\n",
      "100000/100000 [==============================] - 363s 4ms/step - loss: 0.3780 - accuracy: 0.8303 - val_loss: 0.4709 - val_accuracy: 0.7894\n",
      "Epoch 10/50\n",
      "100000/100000 [==============================] - 364s 4ms/step - loss: 0.3608 - accuracy: 0.8387 - val_loss: 0.4798 - val_accuracy: 0.7872\n",
      "Epoch 11/50\n",
      "100000/100000 [==============================] - 364s 4ms/step - loss: 0.3456 - accuracy: 0.8472 - val_loss: 0.4840 - val_accuracy: 0.7876\n",
      "Epoch 12/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.3298 - accuracy: 0.8548 - val_loss: 0.5192 - val_accuracy: 0.7821\n",
      "Epoch 13/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.3155 - accuracy: 0.8619 - val_loss: 0.5170 - val_accuracy: 0.7825\n",
      "Epoch 14/50\n",
      "100000/100000 [==============================] - 366s 4ms/step - loss: 0.3017 - accuracy: 0.8702 - val_loss: 0.5638 - val_accuracy: 0.7812\n",
      "Epoch 15/50\n",
      "100000/100000 [==============================] - 365s 4ms/step - loss: 0.2895 - accuracy: 0.8753 - val_loss: 0.5471 - val_accuracy: 0.7790\n",
      "Epoch 16/50\n",
      "100000/100000 [==============================] - 366s 4ms/step - loss: 0.2782 - accuracy: 0.8806 - val_loss: 0.5620 - val_accuracy: 0.7806\n",
      "Epoch 17/50\n",
      "100000/100000 [==============================] - 365s 4ms/step - loss: 0.2669 - accuracy: 0.8870 - val_loss: 0.5714 - val_accuracy: 0.7799\n",
      "Epoch 18/50\n",
      "100000/100000 [==============================] - 365s 4ms/step - loss: 0.2574 - accuracy: 0.8911 - val_loss: 0.6340 - val_accuracy: 0.7781\n",
      "Epoch 19/50\n",
      "100000/100000 [==============================] - 366s 4ms/step - loss: 0.2496 - accuracy: 0.8946 - val_loss: 0.6163 - val_accuracy: 0.7750\n",
      "Epoch 20/50\n",
      "100000/100000 [==============================] - 366s 4ms/step - loss: 0.2407 - accuracy: 0.8991 - val_loss: 0.6008 - val_accuracy: 0.7786\n",
      "Epoch 21/50\n",
      "100000/100000 [==============================] - 366s 4ms/step - loss: 0.2338 - accuracy: 0.9026 - val_loss: 0.6549 - val_accuracy: 0.7746\n",
      "Epoch 22/50\n",
      "100000/100000 [==============================] - 366s 4ms/step - loss: 0.2273 - accuracy: 0.9046 - val_loss: 0.6458 - val_accuracy: 0.7736\n",
      "Epoch 23/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.2218 - accuracy: 0.9070 - val_loss: 0.6080 - val_accuracy: 0.7758\n",
      "Epoch 24/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.2151 - accuracy: 0.9114 - val_loss: 0.6784 - val_accuracy: 0.7746\n",
      "Epoch 25/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.2134 - accuracy: 0.9124 - val_loss: 0.6582 - val_accuracy: 0.7746\n",
      "Epoch 26/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.2074 - accuracy: 0.9150 - val_loss: 0.6646 - val_accuracy: 0.7731\n",
      "Epoch 27/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.2035 - accuracy: 0.9171 - val_loss: 0.7314 - val_accuracy: 0.7706\n",
      "Epoch 28/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.2001 - accuracy: 0.9177 - val_loss: 0.7283 - val_accuracy: 0.7710\n",
      "Epoch 29/50\n",
      "100000/100000 [==============================] - 379s 4ms/step - loss: 0.1940 - accuracy: 0.9204 - val_loss: 0.7275 - val_accuracy: 0.7703\n",
      "Epoch 30/50\n",
      "100000/100000 [==============================] - 386s 4ms/step - loss: 0.1929 - accuracy: 0.9212 - val_loss: 0.7328 - val_accuracy: 0.7702\n",
      "Epoch 31/50\n",
      "100000/100000 [==============================] - 388s 4ms/step - loss: 0.1892 - accuracy: 0.9226 - val_loss: 0.7387 - val_accuracy: 0.7721\n",
      "Epoch 32/50\n",
      "100000/100000 [==============================] - 381s 4ms/step - loss: 0.1874 - accuracy: 0.9240 - val_loss: 0.7405 - val_accuracy: 0.7704\n",
      "Epoch 33/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1830 - accuracy: 0.9261 - val_loss: 0.7223 - val_accuracy: 0.7697\n",
      "Epoch 34/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1795 - accuracy: 0.9268 - val_loss: 0.7835 - val_accuracy: 0.7723\n",
      "Epoch 35/50\n",
      "100000/100000 [==============================] - 368s 4ms/step - loss: 0.1780 - accuracy: 0.9282 - val_loss: 0.7738 - val_accuracy: 0.7718\n",
      "Epoch 36/50\n",
      "100000/100000 [==============================] - 366s 4ms/step - loss: 0.1752 - accuracy: 0.9295 - val_loss: 0.7871 - val_accuracy: 0.7613\n",
      "Epoch 37/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1721 - accuracy: 0.9308 - val_loss: 0.8391 - val_accuracy: 0.7695\n",
      "Epoch 38/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1733 - accuracy: 0.9298 - val_loss: 0.7481 - val_accuracy: 0.7681\n",
      "Epoch 39/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1677 - accuracy: 0.9331 - val_loss: 0.7690 - val_accuracy: 0.7682\n",
      "Epoch 40/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1670 - accuracy: 0.9329 - val_loss: 0.7882 - val_accuracy: 0.7649\n",
      "Epoch 41/50\n",
      "100000/100000 [==============================] - 369s 4ms/step - loss: 0.1650 - accuracy: 0.9331 - val_loss: 0.8291 - val_accuracy: 0.7689\n",
      "Epoch 42/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1616 - accuracy: 0.9350 - val_loss: 0.8417 - val_accuracy: 0.7697\n",
      "Epoch 43/50\n",
      "100000/100000 [==============================] - 368s 4ms/step - loss: 0.1627 - accuracy: 0.9351 - val_loss: 0.8109 - val_accuracy: 0.7685\n",
      "Epoch 44/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1612 - accuracy: 0.9359 - val_loss: 0.8277 - val_accuracy: 0.7667\n",
      "Epoch 45/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1574 - accuracy: 0.9370 - val_loss: 0.7926 - val_accuracy: 0.7675\n",
      "Epoch 46/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1591 - accuracy: 0.9364 - val_loss: 0.7873 - val_accuracy: 0.7649\n",
      "Epoch 47/50\n",
      "100000/100000 [==============================] - 367s 4ms/step - loss: 0.1555 - accuracy: 0.9383 - val_loss: 0.8280 - val_accuracy: 0.7670\n",
      "Epoch 48/50\n",
      "100000/100000 [==============================] - 382s 4ms/step - loss: 0.1538 - accuracy: 0.9387 - val_loss: 0.8635 - val_accuracy: 0.7692\n",
      "Epoch 49/50\n",
      "100000/100000 [==============================] - 390s 4ms/step - loss: 0.1544 - accuracy: 0.9389 - val_loss: 0.8430 - val_accuracy: 0.7675\n",
      "Epoch 50/50\n",
      "100000/100000 [==============================] - 389s 4ms/step - loss: 0.1547 - accuracy: 0.9387 - val_loss: 0.8901 - val_accuracy: 0.7692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x163626990>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X_indices, train_Y,\n",
    "          batch_size=32,\n",
    "          epochs=50,\n",
    "          validation_data=(val_X_indices, val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 10s 992us/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_X_indices, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7646999955177307\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SentClass7model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentClass7_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SentClass7json.json\", \"w\") as json_file:\n",
    "    json_file.write(SentClass7_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('SentClass7weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
